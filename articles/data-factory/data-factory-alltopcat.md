---
title: Todos los temas del servicio Data Factory | Microsoft Docs
description: Tabla de todos los temas el servicio de Azure denominado Data Factory disponibles en http://azure.microsoft.com/documentation/articles/, título y descripción.
services: data-factory
documentationcenter: ''
author: spelluru
manager: jhubbard
editor: MightyPen

ms.service: data-factory
ms.workload: data-factory
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 10/05/2016
ms.author: spelluru

---
# <a name="all-topics-for-azure-data-factory-service"></a>Todos los temas del servicio Data Factory de Azure
En este tema se muestran todos los temas que se aplican directamente al servicio **Data Factory** de Azure. Puede buscar en esta página web las palabras clave mediante **Ctrl+F**, para encontrar los temas de interés.

## <a name="new"></a>Nuevo
| &nbsp; | Título | Descripción |
| ---:|:--- |:--- |
| 1 |[Movimiento de datos de Amazon Redshift mediante Azure Data Factory](data-factory-amazon-redshift-connector.md) |Obtenga información sobre cómo mover datos desde Amazon Redshift mediante Azure Data Factory. |
| 2 |[Movimiento de datos desde Amazon Simple Storage Service mediante Azure Data Factory](data-factory-amazon-simple-storage-service-connector.md) |Aprenda a mover datos desde Amazon Simple Storage Service (S3) mediante Azure Data Factory. |
| 3 |[Asistente para copia de Azure Data Factory](data-factory-azure-copy-wizard.md) |Obtenga información sobre cómo utilizar el Asistente para copia de Azure Data Factory para copiar datos de orígenes de datos admitidos en receptores. |
| 4 |[Tutorial: Compilación de la primera instancia de Azure Data Factory con la API de REST de Data Factory](data-factory-build-your-first-pipeline-using-rest-api.md) |En este tutorial se crea un ejemplo de canalización de Data Factory de Azure con la API de REST de Data Factory. |
| 5 |[Tutorial: crear una canalización con la actividad de copia mediante la API de NET](data-factory-copy-activity-tutorial-using-dotnet-api.md) |En este tutorial, se crea una canalización de Data Factory de Azure con una actividad de copia mediante la API de .NET. |
| 6 |[Tutorial: Crear una canalización con la actividad de copia mediante la API de REST](data-factory-copy-activity-tutorial-using-rest-api.md) |En este tutorial, creará una canalización de Data Factory de Azure con una actividad de copia mediante Visual Studio. |
| 7 |[Asistente para copia de Data Factory](data-factory-copy-wizard.md) |Obtenga información sobre cómo utilizar el Asistente para copia de Data Factory para copiar datos de orígenes de datos admitidos en receptores. |
| 8 |[Data Management Gateway](data-factory-data-management-gateway.md) |Configuración de una puerta de enlace para mover datos entre una infraestructura local y la nube. Uso de Data Management Gateway en Data Factory de Azure para mover los datos. |
| 9 |[Movimiento de datos desde una base de datos de Cassandra local con Data Factory de Azure](data-factory-onprem-cassandra-connector.md) |Aprenda a mover los datos de una base de datos de Cassandra local con Data Factory de Azure. |
| 10 |[Movimiento de datos de MongoDB mediante Data Factory de Azure](data-factory-on-premises-mongodb-connector.md) |Obtenga información acerca de cómo mover los datos de la base de datos de MongoDB mediante Data Factory de Azure. |
| 11 |[Movimiento de datos de Salesforce mediante el uso de Data Factory de Azure](data-factory-salesforce-connector.md) |Aprenda a mover datos de Salesforce usando Data Factory de Azure. |

## <a name="updated-articles,-data-factory"></a>Artículos actualizados, Data Factory
En esta sección se enumeran los artículos que se han actualizado recientemente. Concretamente, aquellos en los que la actualización fuera grande o considerable. Para cada artículo actualizado, se muestra un fragmento del texto agregado. Los artículos se actualizaron dentro de este intervalo de fechas: del **22-08-2016** al **05-10-2016**.

| &nbsp; | Artículo | Texto actualizado, fragmento | Se actualiza cuando |
| ---:|:--- |:--- |:--- |
| 12 |[Factoría de datos de Azure: registro de cambios de la API de .NET](data-factory-api-change-log.md) |En este artículo se proporciona información sobre los cambios realizados en el SDK de Factoría de datos de Azure en una versión específica. El paquete NuGet más reciente para Azure Data Factory se encuentra aquí (https://www.nuget.org/packages/Microsoft.Azure.Management.DataFactories) ** Version 4.11.0** Incorporación de características: / Se han agregado los siguientes tipos de servicios vinculados:  -  OnPremisesMongoDbLinkedService (https://msdn.microsoft.com/library/mt765129.aspx)    -  AmazonRedshiftLinkedService (https://msdn.microsoft.com/library/mt765121.aspx)   -  AwsAccessKeyLinkedService (https://msdn.microsoft.com/library/mt765144.aspx) / Se han agregado los siguientes tipos de conjuntos de datos:  -  MongoDbCollectionDataset (https://msdn.microsoft.com/library/mt765145.aspx)  -  AmazonS3Dataset (https://msdn.microsoft.com/library/mt765112.aspx) / Se han agregado los siguientes tipos de orígenes de copia:    -  MongoDbSource (https://msdn.microsoft.com/en-US/library/mt765123.aspx) ** Versión 4.10.0** / Se han agregado las siguientes propiedades opcionales a TextFormat:    -  Ski |2016-09-22 |
| 13 |[Movimiento de datos hacia y desde Blob de Azure mediante Factoría de datos de Azure](data-factory-azure-blob-connector.md) |/  copyBehavior  / Define el comportamiento de copia cuando el origen es BlobSource o FileSystem.  /  **PreserveHierarchy:** conserva la jerarquía de archivos en la carpeta de destino. La ruta de acceso relativa del archivo de origen que apunta a la carpeta de origen es idéntica a la ruta de acceso relativa del archivo de destino que apunta a la carpeta de destino..br/..br/.**FlattenHierarchy:** todos los archivos de la carpeta de origen están en el primer nivel de la carpeta de destino. Los archivos de destino tienen un nombre generado automáticamente. .br/..br/.**MergeFiles: (valor predeterminado)** combina todos los archivos de la carpeta de origen en un solo archivo. Si se especifica el nombre de archivo/blob, el nombre de archivo combinado sería el nombre especificado; de lo contrario, sería el nombre de archivo generado automáticamente.  /  No  /  **BlobSource** también admite estas dos propiedades para ofrecer compatibilidad con versiones anteriores. / **treatEmptyAsNull**: especifica si se debe tratar una cadena nula o vacía como un valor nulo. / **skipHeaderLineCount** : especifica cuántas líneas deben omitirse. Es aplicable únicamente cuando el conjunto de datos de entrada usa TextFormat. De forma similar, **BlobSink** admite th |2016-09-28 |
| 14 |[Creación de canalizaciones predictivas con las actividades de Aprendizaje automático de Azure](data-factory-azure-ml-batch-execution-activity.md) |** El servicio web requiere varias entradas** Si el servicio web toma varias entradas, use la propiedad **webServiceInputs** en lugar de usar **webServiceInput**. Los conjuntos de datos a los que hace referencia **webServiceInputs** también deben incluirse en las **entradas** de la actividad. En el experimento de Azure ML, la entrada del servicio web y los puertos de salida y parámetros globales tienen nombres predeterminados (input1 e input2) que se pueden personalizar. Los nombres que se utilizan para la configuración de globalParameters, webServiceOutputs y webServiceInputs deben coincidir exactamente con los de los experimentos. Puede ver la carga útil de la solicitud de ejemplo en la página de ayuda de ejecución de lotes del punto de conexión de Azure ML para comprobar la asignación esperada.    {       "name": "PredictivePipeline",       "properties": {             "description": "use AzureML model",             "activities":  {                "name": "MLActivity",               "type": "AzureMLBatchExecution",                "description": "prediction analysis on batch input",                "inputs":  {                    "name": "inputDataset1"                 }, {                    "name": "inputDatase |2016-09-13 |
| 15 |[Guía de optimización y rendimiento de la actividad de copia](data-factory-copy-activity-performance.md) |1. **Establezca una línea base**. Durante la fase de desarrollo, pruebe la canalización usando la actividad de copia en un ejemplo de datos representativo. Puede utilizar el modelo de segmentación de Data Factory (data-factory-scheduling-and-execution.md time-series-datasets-and-data-slices) para limitar la cantidad de datos con los que trabaja.  Recopile características de tiempo de ejecución y rendimiento mediante la **Aplicación de supervisión y administración**. Elija **Supervisión y administración** en la página de inicio de Data Factory. En la vista de árbol, elija el **conjunto de datos de salida**. En la lista **Activity Windows** (Ventanas de actividad), elija la ejecución de la actividad de copia. **Activity Windows** (Ventanas de actividad) se muestra la duración de la actividad de copia y el tamaño de los datos que se copian. El rendimiento se muestra en **Activity Window Explorer**(Explorador de ventanas de actividad). Para más información sobre la aplicación, consulte Supervisión y administración de canalizaciones de Azure Data Factory mediante la nueva Aplicación de supervisión y administración (data-factory-monitor-manage-app.md).  ! Detalles de ejecución de actividad (./media/data-factory-copy-activity-performance/mmapp-activity-run-details.pn |2016-09-27 |
| 16 |[Tutorial: Crear una canalización con la actividad de copia mediante Visual Studio](data-factory-copy-activity-tutorial-using-visual-studio.md) |Tenga en cuenta los siguientes puntos:  -el **type** de conjunto de datos se establece en **AzureBlob**.     - **linkedServiceName** está establecido en **AzureStorageLinkedService**. Este servicio vinculado lo creó en el paso 2.     **folderPath** está establecido en el contenedor **adftutorial**. También puede especificar el nombre de un blob en la carpeta mediante la propiedad **fileName** . Puesto que no se especifica el nombre del blob, los datos de todos los blobs del contenedor se consideran datos de entrada.    - el **tipo** de formato se establece en **TextFormat**  - Hay dos campos en el archivo de texto ΓÇô **FirstName** y **LastName** ΓÇô separados por un carácter de coma (**columnDelimiter**): la **disponibilidad** está establecida en **cada hora** (la **frecuencia** está establecida en **hora** y el **intervalo** está establecido en **1**). Por consiguiente, Data Factory busca los datos de entrada cada hora en la carpeta raíz del contenedor de blobs (**adftutorial**) que se ha especificado.  Si no especifica un valor de **fileName** para un conjunto de datos de **entrada**, todos los archivos o blobs de la carpeta de entrada (**folderPath**) se consideran entradas. |2016-09-29 |
| 17 |[Crear, supervisar y administrar factorías de datos de Azure mediante el SDK de .NET de la factoría de datos](data-factory-create-data-factories-programmatically.md) |Anote el id. de aplicación y la contraseña (secreto de cliente) y úselos en el tutorial. ** Obtención de los id. de suscripción y de inquilino de Azure**Si no tiene instalada la última versión de PowerShell en su máquina, siga las instrucciones del artículo Cómo instalar y configurar Azure PowerShell (../powershell-install-configure.md) para hacerlo. 1. Inicie Azure PowerShell y ejecute el comando siguiente. 2 Ejecute el siguiente comando y escriba el nombre de usuario y la contraseña que utiliza para iniciar sesión en el Portal de Azure.         Login-AzureRmAccount   Si solo tiene una suscripción de Azure asociada con esta cuenta, no es necesario que realice los dos pasos siguientes. 3. Ejecute el siguiente comando para ver todas las suscripciones para esta cuenta.       Get-AzureRmSubscription 4. Ejecute el comando siguiente para seleccionar la suscripción con la que desea trabajar. Reemplace **NameOfAzureSubscription** por el nombre de su suscripción de Azure.       Get-AzureRmSubscription -SubscriptionName NameOfAzureSubscription  /  Set-AzureRmCo |2016-09-14 |
| 18 |[Canalizaciones y actividades en Azure Data Factory](data-factory-create-pipelines.md) |,       "start": "2016-07-12T00:00:00Z",    "end": "2016-07-13T00:00:00Z"       }     } Tenga en cuenta los siguientes puntos: / en la sección de actividades, hay solo una actividad cuyo **type** está establecido en **Copy**. La entrada de la actividad está establecida en **InputDataset**, mientras que la salida está establecida en **OutputDataset**. / En la sección **typeProperties**, **BlobSource** se especifica como el tipo de origen y **SqlSink** como el tipo de receptor. Para obtener un tutorial completo de creación de esta canalización, consulte Tutorial: Copiar datos desde Blob Storage a SQL Database (data-factory-copy-data-from-azure-blob-storage-to-sql-database.md). ** Canalización de transformación de ejemplo** En la canalización de ejemplo siguiente, hay una actividad del tipo **HDInsightHive** en la sección de **actividades**. En este ejemplo, la actividad de HDInsight Hive (data-factory-hive-activity.md) transforma los datos de Azure Blob Storage mediante la ejecución de un archivo de script de Hive en un clúster de Azure HDInsight Hadoop.  {     "name": "TransformPipeline",    "p |2016-09-27 |
| 19 |[Transformar datos en Azure Data Factory](data-factory-data-transformation-activities.md) |Data Factory admite las siguientes actividades de transformación de datos que se pueden agregar a las canalizaciones (data-factory-create-pipelines.md) tanto individualmente como encadenadas a otra actividad. .  AZURE.NOTE  Para obtener un tutorial con instrucciones paso a paso, consulte el artículo Crear una canalización con la transformación de Hive (data-factory-build-your-first-pipeline.md). ** Actividad de HDInsight Hive** La actividad de HDInsight Hive en una canalización de Data Factory ejecuta consultas de Hive en su propio clúster de HDInsight o en uno a petición basado en Windows/Linux. Consulte el artículo Actividad de Hive (data-factory-hive-activity.md) para más información acerca de esta actividad. ** Actividad de HDInsight Pig** La actividad de HDInsight Pig en una canalización de Data Factory ejecuta consultas de Pig en su propio clúster de HDInsight o en uno a petición basado en Windows/Linux. Consulte el artículo Actividad de Pig (data-factory-hive-activity.md) para más información acerca de esta actividad. ** Actividad de MapReduce de HDInsight** La actividad de MapReduce de HDInsight en una canalización de Data Factory ejecuta programas de MapReduce en su propio clúster de HDInsight o en uno basado en Windows/Linux a petición. |2016-09-26 |
| 20 | |[Programación y ejecución de Data Factory](data-factory-scheduling-and-execution.md) |ActividadCopia2 solo se ejecutaría si ActividadCopia1 se hubiera ejecutado correctamente y ConjuntoDatos2 estuviera disponible. Este es el JSON de la canalización de ejemplo:  {       "name": "ChainActivities",    "properties": {           "description": "Run activities in sequence",      "activities":       {       "type": "Copy",     "typeProperties": {     "source": {     "type": "BlobSource"    },      "sink": {       "type": "BlobSink",     "copyBehavior": "PreserveHierarchy",    "writeBatchSize": 0,    "writeBatchTimeout": "00:00:00"     }       },      "inputs":       {       "name": "Dataset1"      }       ,       "outputs":      {       "name": "Dataset2"      }       ,       "policy": {     "timeout": "01:00:00"       },      "scheduler": {      "frequency": "Hour",    "interval": 1       },      "name": "CopyFromBlob1ToBlob2",     "description": "Copy data from a blob to another"       },      {       "type": "Copy",     "typeProperties": {     "source": {     "type": "BlobSource"    },      "sink": {       "type": "BlobSink",     "writeBatchSize": 0,    "writeBatchTimeout": "00:00:00"     }       },      "in |

## <a name="tutorials"></a>Tutoriales
| &nbsp; | Título | Descripción |
| ---:|:--- |:--- |
| 21 |[Tutorial: Compilación de la primera canalización para procesar datos mediante el clúster de Hadoop](data-factory-build-your-first-pipeline.md) |En este tutorial de Data Factory de Azure se muestra cómo crear y programar una factoría de datos que procese los datos mediante el script de Hive en un clúster de Hadoop. |
| 22 |[Tutorial: Compilación de la primera Data Factory de Azure con la plantilla de Azure Resource Manager](data-factory-build-your-first-pipeline-using-arm.md) |En este tutorial, creará una canalización de Azure Data Factory de ejemplo con la plantilla de Azure Resource Manager. |
| 23 |[Tutorial: Compilación de la primera instancia de Azure Data Factory con Azure Portal](data-factory-build-your-first-pipeline-using-editor.md) |En este tutorial, se crea una canalización de Data Factory de Azure de ejemplo con el Editor de Data Factory en el Portal de Azure. |
| 24 |[Tutorial: Compilación de la primera instancia de Azure Data Factory con Azure PowerShell](data-factory-build-your-first-pipeline-using-powershell.md) |En este tutorial, creará un de ejemplo de canalización de Data Factory de Azure mediante Azure PowerShell. |
| 25 |[Tutorial: Compilación de la primera instancia de Azure Data Factory con Microsoft Visual Studio](data-factory-build-your-first-pipeline-using-vs.md) |En este tutorial, creará un de ejemplo de canalización de Data Factory de Azure mediante Visual Studio. |
| 26 |[Tutorial: Crear una canalización con la actividad de copia mediante Azure Portal](data-factory-copy-activity-tutorial-using-azure-portal.md) |En este tutorial, creará una canalización de Data Factory de Azure con una actividad de copia mediante el Editor de Data Factory en el Portal de Azure. |
| 27 |[Tutorial: Crear una canalización con la actividad de copia con Azure PowerShell](data-factory-copy-activity-tutorial-using-powershell.md) |En este tutorial, creará una canalización de Data Factory de Azure con una actividad de copia mediante Azure PowerShell. |
| 28 |[Tutorial: Crear una canalización con la actividad de copia mediante Visual Studio](data-factory-copy-activity-tutorial-using-visual-studio.md) |En este tutorial, creará una canalización de Data Factory de Azure con una actividad de copia mediante Visual Studio. |
| 29 |[Copia de datos de Almacenamiento de blobs en Base de datos SQL mediante Data Factory](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md) |En este tutorial se muestra cómo usar la actividad de copia en una canalización de Data Factory de Azure para copiar datos de Almacenamiento de blobs en Base de datos SQL de Azure. |
| 30 |[Tutorial: crear una canalización con la actividad de copia mediante el Asistente para copia de Data Factory](data-factory-copy-data-wizard-tutorial.md) |En este tutorial, creará una canalización de Data Factory de Azure con una actividad de copia mediante el Asistente para copia compatible con Data Factory. |

## <a name="data-movement"></a>Movimiento de datos
| &nbsp; | Título | Description |
| ---:|:--- |:--- |
| 31 |[Movimiento de datos hacia y desde Blob de Azure mediante Factoría de datos de Azure](data-factory-azure-blob-connector.md) |Aprenda a copiar datos de blob en Data Factory de Azure. Use nuestro ejemplo: Copia de datos entre Almacenamiento de blobs de Azure y Base de datos SQL de Azure. |
| 32 |[Movimiento de datos hacia y desde el almacén de Azure Data Lake mediante la Factoría de datos de Azure](data-factory-azure-datalake-connector.md) |Obtenga información sobre cómo mover datos hacia y desde el almacén de Azure Data Lake mediante la Factoría de datos de Azure. |
| 33 |[Movimiento de datos hacia y desde DocumentDB mediante Factoría de datos de Azure](data-factory-azure-documentdb-connector.md) |Obtenga información acerca de cómo mover los datos hacia y desde DocumentDB de Azure mediante Factoría de datos de Azure |
| 34 |[Movimiento de datos hacia y desde Base de datos SQL de Azure mediante Factoría de datos de Azure](data-factory-azure-sql-connector.md) |Obtenga información acerca de cómo mover los datos hacia y desde la base de datos SQL de Azure mediante Factoría de datos de Azure. |
| 35 |[Movimiento de datos hacia y desde Almacenamiento de datos SQL de Azure mediante Factoría de datos de Azure](data-factory-azure-sql-data-warehouse-connector.md) |Obtenga información acerca de cómo mover los datos hacia y desde Almacenamiento de datos SQL de Azure mediante Factoría de datos de Azure |
| 36 |[Movimiento de datos hacia y desde Tabla de Azure mediante Factoría de datos de Azure](data-factory-azure-table-connector.md) |Obtenga información acerca de cómo mover los datos hacia y desde Almacenamiento de tablas de Azure mediante Factoría de datos de Azure. |
| 37 |[Guía de optimización y rendimiento de la actividad de copia](data-factory-copy-activity-performance.md) |Conozca los factores más importantes que afectan al rendimiento del movimiento de datos en Data Factory de Azure cuando se usa la actividad de copia. |
| 38 |[Movimiento de datos con la actividad de copia](data-factory-data-movement-activities.md) |Aprenda sobre el movimiento de datos en las canalizaciones de Data Factory: migración de datos entre almacenes en la nube, entre un almacén de datos local y un almacén de datos en la nube. Utilice la actividad de copia. |
| 39 |[Notas de la versión de Data Management Gateway](data-factory-gateway-release-notes.md) |Notas de la versión de Data Management Gateway |
| 40 |[Movimiento de datos desde HDFS local mediante Factoría de datos de Azure](data-factory-hdfs-connector.md) |Obtenga información acerca de cómo mover datos desde HDFS local mediante Factoría de datos de Azure |
| 41 |[Supervisión y administración de canalizaciones de Data Factory de Azure mediante la nueva Aplicación de supervisión y administración](data-factory-monitor-manage-app.md) |Obtenga información sobre cómo usar la Aplicación de supervisión y administración para supervisar y administrar factorías de datos y canalizaciones de Azure |
| 42 |[Movimiento de datos entre orígenes locales y la nube con Data Management Gateway](data-factory-move-data-between-onprem-and-cloud.md) |Configuración de una puerta de enlace para mover datos entre una infraestructura local y la nube. Uso de Data Management Gateway en Data Factory de Azure para mover los datos. |
| 43 |[Movimiento de datos de un origen de OData mediante Factoría de datos de Azure](data-factory-odata-connector.md) |Obtenga información sobre cómo mover datos desde orígenes de OData mediante Factoría de datos de Azure. |
| 44 |[Movimiento de datos desde almacenes de datos ODBC mediante Factoría de datos de Azure](data-factory-odbc-connector.md) |Obtenga información sobre cómo mover datos desde almacenes de datos ODBC mediante Factoría de datos de Azure. |
| 45 |[Movimiento de datos de DB2 mediante Factoría de datos de Azure](data-factory-onprem-db2-connector.md) |Obtenga información acerca de cómo mover los datos de la base de datos de DB2 mediante Factoría de datos de Azure |
| 46 |[Movimiento de datos hacia el sistema de archivos local y desde él con Azure Data Factory](data-factory-onprem-file-system-connector.md) |Aprenda a mover datos hacia el sistema de archivos local y desde él con Azure Data Factory. |
| 47 |[Movimiento de datos de MySQL mediante Factoría de datos de Azure](data-factory-onprem-mysql-connector.md) |Obtenga información acerca de cómo mover los datos de la base de datos de MySQL mediante Factoría de datos de Azure. |
| 48 |[Transferencia de datos de/a Oracle local mediante Data Factory de Azure](data-factory-onprem-oracle-connector.md) |Obtenga información acerca de cómo mover los datos hacia y desde la base de datos de Oracle local mediante Factoría de datos de Azure. |
| 49 |[Movimiento de datos de PostgreSQL mediante Factoría de datos de Azure](data-factory-onprem-postgresql-connector.md) |Obtenga información acerca de cómo mover los datos de la base de datos de PostgreSQL mediante Factoría de datos de Azure. |
| 50 |[Movimiento de datos de Sybase mediante Factoría de datos de Azure](data-factory-onprem-sybase-connector.md) |Obtenga información acerca de cómo mover los datos de la base de datos de Sybase mediante Factoría de datos de Azure. |
| 51 |[Movimiento de datos de Teradata mediante Factoría de datos de Azure](data-factory-onprem-teradata-connector.md) |Obtenga información acerca del conector Teradata para el servicio Factoría de datos que le permite mover datos desde Base de datos Teradata. |
| 52 |[Movimiento de los datos entre entornos locales de SQL Server o en IaaS (máquina virtual de Azure) mediante Factoría de datos de Azure](data-factory-sqlserver-connector.md) |Aprenda a mover los datos hacia y desde una base de datos SQL Server en un entorno local o en una máquina virtual de Azure mediante Factoría de datos de Azure. |
| 53 |[Movimiento de datos de un origen de tabla web mediante Factoría de datos de Azure](data-factory-web-table-connector.md) |Obtenga información sobre cómo mover datos desde una tabla local en una página web mediante Factoría de datos de Azure |

## <a name="data-transformation"></a>Transformación de datos
| &nbsp; | Título | Description |
| ---:|:--- |:--- |
| 54 |[Creación de canalizaciones predictivas con las actividades de Aprendizaje automático de Azure](data-factory-azure-ml-batch-execution-activity.md) |Describe cómo crear canalizaciones predictivas con Factoría de datos de Azure y Aprendizaje automático de Azure |
| 55 |[Servicios vinculados de procesos](data-factory-compute-linked-services.md) |Obtenga información sobre los entornos de procesos que puede usar en las canalizaciones de la Factoría de datos de Azure para transformar y procesar datos. |
| 56 |[Procesamiento de datos a gran escala mediante Data Factory y Lote](data-factory-data-processing-using-batch.md) |Describe cómo procesar grandes cantidades de datos en una canalización de Factoría de datos de Azure mediante la funcionalidad de procesamiento paralelo de Lote de Azure. |
| 57 |[Transformar datos en Azure Data Factory](data-factory-data-transformation-activities.md) |Aprenda cómo transformar datos o procesar datos en Azure Data Factory mediante Hadoop, Machine Learning o Azure Data Lake Analytics. |
| 58 |[Actividad de streaming de Hadoop](data-factory-hadoop-streaming-activity.md) |Aprenda cómo puede usar la actividad de streaming de Hadoop en una factoría de datos de Azure para ejecutar programas de streaming de Hadoop en un clúster de HDInsight suyo propio o a petición. |
| 59 |[Actividad de Hive](data-factory-hive-activity.md) |Aprenda cómo puede usar la actividad de Hive en Factoría de datos de Azure para ejecutar consultas de Hive en un clúster de HDInsight suyo propio o a petición. |
| 60 |[Invocar programas MapReduce desde la factoría de datos de Azure](data-factory-map-reduce.md) |Obtenga información sobre cómo procesar datos mediante la ejecución de programas MapReduce en un clúster de HDInsight de Azure desde una factoría de datos de Azure. |
| 61 |[Actividad de Pig](data-factory-pig-activity.md) |Aprenda cómo puede usar la actividad de Pig en Factoría de datos de Azure para ejecutar scripts de Pig en un clúster de HDInsight suyo propio o a petición. |
| 62 |[Invocar programas Spark desde Data Factory](data-factory-spark.md) |Obtenga información sobre cómo invocar programas Spark desde Data Factory de Azure mediante la actividad MapReduce. |
| 63 |[Actividad de procedimiento almacenado de SQL Server](data-factory-stored-proc-activity.md) |Sepa cómo usar la actividad de procedimiento almacenado de SQL Server para invocar un procedimiento almacenado en una Base de datos SQL de Azure o en un Almacenamiento de datos SQL de Azure desde una canalización de Factoría de datos. |
| 64 |[Uso de actividades personalizadas en una canalización de Factoría de datos de Azure](data-factory-use-custom-activities.md) |Obtenga información acerca de cómo crear actividades personalizadas y usarlas en una canalización de la factoría de datos de Azure. |
| 65 |[Ejecución del script de U-SQL en Análisis de Azure Data Lake desde Factoría de datos de Azure](data-factory-usql-activity.md) |Aprenda a procesar datos ejecutando scripts U-SQL en el servicio de proceso de Análisis de Azure Data Lake. |

## <a name="samples"></a>Muestras
| &nbsp; | Título | Descripción |
| ---:|:--- |:--- |
| 66 |[Factoría de datos de Azure: ejemplos](data-factory-samples.md) |Proporciona detalles sobre ejemplos que se distribuyen con el servicio Factoría de datos de Azure. |

## <a name="use-cases"></a>Casos de uso
| &nbsp; | Título | Descripción |
| ---:|:--- |:--- |
| 67 |[Casos prácticos de clientes](data-factory-customer-case-studies.md) |Obtenga información sobre la manera en que algunos de nuestros clientes han venido utilizando la Factoría de datos de Azure. |
| 68 |[Caso de uso - Generación de perfiles de clientes](data-factory-customer-profiling-usecase.md) |Obtenga información sobre cómo se usa Factoría de datos de Azure para crear un flujo de trabajo orientado a datos (canalización) para generar perfiles de clientes de juegos. |
| 69 |[Caso de uso: recomendaciones de productos](data-factory-product-reco-usecase.md) |Obtenga información acerca de un caso de uso que se implementan mediante Factoría de datos de Azure junto con otros servicios. |

## <a name="monitor-and-manage"></a>Supervisar y administrar
| &nbsp; | Título | Description |
| ---:|:--- |:--- |
| 70 |[Supervisión y administración de canalizaciones de la Factoría de datos de Azure](data-factory-monitor-manage-pipelines.md) |Obtenga información sobre el uso del Portal de Azure y Azure PowerShell para supervisar y administrar las factorías de datos y las canalizaciones de Azure que haya creado. |

## <a name="sdk"></a>SDK
| &nbsp; | Título | Descripción |
| ---:|:--- |:--- |
| 71 |[Factoría de datos de Azure: registro de cambios de la API de .NET](data-factory-api-change-log.md) |Describe los cambios importantes, las adiciones de características y las correcciones de errores, entre otros, en una versión específica de la API de .NET para Factoría de datos de Azure. |
| 72 |[Crear, supervisar y administrar factorías de datos de Azure mediante el SDK de .NET de la factoría de datos](data-factory-create-data-factories-programmatically.md) |Conozca cómo puede crear, supervisar y administrar factorías de datos de Azure mediante programación con el SDK de Factoría de datos. |
| 73 |[Referencia para desarrolladores de Factoría de datos de Azure](data-factory-sdks.md) |Obtenga información sobre las distintas formas de crear, supervisar y administrar factorías de datos de Azure. |

## <a name="miscellaneous"></a>Varios
| &nbsp; | Título | Descripción |
| ---:|:--- |:--- |
| 74 |[Factoría de datos de Azure: preguntas más frecuentes](data-factory-faq.md) |Preguntas más frecuentes acerca de la factoría de datos de Azure. |
| 75 |[Data Factory de Azure: funciones y variables del sistema](data-factory-functions-variables.md) |Proporciona una lista de funciones y variables del sistema de Data Factory de Azure |
| 76 |[Factoría de datos de Azure: reglas de nomenclatura](data-factory-naming-rules.md) |Describe las reglas de nomenclatura para las entidades de Factoría de datos. |
| 77 |[Solución de problemas de la factoría de datos](data-factory-troubleshoot.md) |Obtenga información acerca de la solución de problemas relacionados con la factoría de datos de Azure. |

<!--HONumber=Oct16_HO2-->


